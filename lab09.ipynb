{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efb1385-819d-4d23-adf3-c4d6651baa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd2d02a-aa86-42d4-9ce0-a3b8a1501f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "gef = np.loadtxt('GEFCOM.txt')\n",
    "cols = ['date', 'hour', 'price', 'sysload', 'zoneload', 'dotw']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f94af4-c79b-4608-9af7-8335ab2151c7",
   "metadata": {},
   "source": [
    "After loading the data, we create two forecasts - AR(1) with single estimation in two variants: jointly and separately for all hours of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5392c7-1cc3-41b7-a80a-430cb0310e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Least-squares jointly:  8.37476435765813\n",
      "MAE of Least-squares separately:  8.259542237368105\n"
     ]
    }
   ],
   "source": [
    "# base cases - AR(1) model modeled jointly for all hours of the day and separately and jointly\n",
    "X = gef[:, 2:4].copy()\n",
    "X[:, 1] = 1\n",
    "X[:, 0] = np.nan\n",
    "X[24:, 0] = gef[:-24, 2] # the first lag of the time series is inserted into the X matrix, starting at the 24th row\n",
    "Y = gef[:, 2].copy() # the dependent variable Y is set to the second column of the gef array\n",
    "firstind = 24\n",
    "lastind = np.argwhere(gef[:, 0] == 20130101).squeeze()[0] # searching for the first occurrence of the date 20130101 in the first column of the gef array\n",
    "# squeeze() method is used to convert the resulting one-dimensional array to a scalar\n",
    "Xt = X[firstind:lastind]\n",
    "Xf = X[lastind:]\n",
    "Yt = Y[firstind:lastind]\n",
    "Yf = Y[lastind:]\n",
    "betas = np.linalg.lstsq(Xt, Yt, rcond=None)[0]\n",
    "betas\n",
    "forecast = np.dot(Xf, betas) # matrix multiplying the test set Xf by the estimated coefficients betas\n",
    "forecast.shape == Yf.shape\n",
    "print('MAE of Least-squares jointly: ', np.mean(np.abs(forecast - Yf)))\n",
    "\n",
    "separate = np.zeros_like(forecast) # all elements initialized to zero, the same shape and data type as forecast\n",
    "for h in range(24): # model is fit separately for each hour of the day\n",
    "    betas = np.linalg.lstsq(Xt[h::24], Yt[h::24], rcond=None)[0]\n",
    "    separate[h::24] = np.dot(Xf[h::24], betas)\n",
    "print('MAE of Least-squares separately: ', np.mean(np.abs(separate - Yf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bddcb-f409-424e-a209-d6e87c572d84",
   "metadata": {},
   "source": [
    "The first NN model\n",
    "------------------\n",
    "\n",
    "Recreate the joint AR(1) using a Neural Network.\n",
    "\n",
    "Note, that we are using a simple architecture, with no hidden layers. Rerun the cell below multiple times and see, that the error metric changes each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2509dd1c-5745-4a60-839b-ef0bd438d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/75\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 271.7316 - mae: 11.1995 - val_loss: 252.3918 - val_mae: 10.3432\n",
      "Epoch 2/75\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 233.3733 - mae: 9.4846 - val_loss: 218.8917 - val_mae: 8.8383\n",
      "Epoch 3/75\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 211.2326 - mae: 8.2902 - val_loss: 200.5518 - val_mae: 7.8822\n",
      "Epoch 4/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 200.0588 - mae: 7.5298 - val_loss: 190.6814 - val_mae: 7.2970\n",
      "Epoch 5/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 195.0022 - mae: 7.0746 - val_loss: 185.6646 - val_mae: 6.9643\n",
      "Epoch 6/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 193.0749 - mae: 6.8200 - val_loss: 183.2008 - val_mae: 6.7867\n",
      "Epoch 7/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 192.1537 - mae: 6.7331 - val_loss: 182.4460 - val_mae: 6.7303\n",
      "Epoch 8/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 191.7350 - mae: 6.6694 - val_loss: 181.5249 - val_mae: 6.6573\n",
      "Epoch 9/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 191.5594 - mae: 6.6137 - val_loss: 181.0612 - val_mae: 6.6204\n",
      "Epoch 10/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 191.3882 - mae: 6.5853 - val_loss: 180.8677 - val_mae: 6.6075\n",
      "Epoch 11/75\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 191.2513 - mae: 6.5837 - val_loss: 180.7704 - val_mae: 6.6037\n",
      "Epoch 12/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 191.0477 - mae: 6.5783 - val_loss: 180.7787 - val_mae: 6.6095\n",
      "Epoch 13/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 190.9212 - mae: 6.5608 - val_loss: 180.4715 - val_mae: 6.5871\n",
      "Epoch 14/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 190.6809 - mae: 6.5645 - val_loss: 180.6300 - val_mae: 6.6062\n",
      "Epoch 15/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 190.5356 - mae: 6.5553 - val_loss: 180.1715 - val_mae: 6.5713\n",
      "Epoch 16/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 190.3249 - mae: 6.5521 - val_loss: 180.2370 - val_mae: 6.5826\n",
      "Epoch 17/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 190.1340 - mae: 6.5460 - val_loss: 180.1156 - val_mae: 6.5775\n",
      "Epoch 18/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 189.9368 - mae: 6.5215 - val_loss: 179.8770 - val_mae: 6.5624\n",
      "Epoch 19/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 189.6900 - mae: 6.5297 - val_loss: 179.7701 - val_mae: 6.5588\n",
      "Epoch 20/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 189.6471 - mae: 6.5037 - val_loss: 179.8327 - val_mae: 6.5691\n",
      "Epoch 21/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 189.3514 - mae: 6.5110 - val_loss: 179.6827 - val_mae: 6.5617\n",
      "Epoch 22/75\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 189.1628 - mae: 6.5065 - val_loss: 179.2978 - val_mae: 6.5355\n",
      "Epoch 23/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 188.8812 - mae: 6.5134 - val_loss: 179.4184 - val_mae: 6.5506\n",
      "Epoch 24/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 188.6657 - mae: 6.5119 - val_loss: 179.0951 - val_mae: 6.5302\n",
      "Epoch 25/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 188.5783 - mae: 6.4582 - val_loss: 178.9158 - val_mae: 6.5208\n",
      "Epoch 26/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 188.2486 - mae: 6.4672 - val_loss: 178.5666 - val_mae: 6.4984\n",
      "Epoch 27/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 188.0184 - mae: 6.4899 - val_loss: 178.7667 - val_mae: 6.5192\n",
      "Epoch 28/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 187.7487 - mae: 6.4669 - val_loss: 178.4057 - val_mae: 6.4967\n",
      "Epoch 29/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 187.5533 - mae: 6.4407 - val_loss: 178.3553 - val_mae: 6.4977\n",
      "Epoch 30/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 187.4771 - mae: 6.4768 - val_loss: 178.4529 - val_mae: 6.5095\n",
      "Epoch 31/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 187.1588 - mae: 6.4297 - val_loss: 177.7915 - val_mae: 6.4654\n",
      "Epoch 32/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 187.0030 - mae: 6.4390 - val_loss: 178.2916 - val_mae: 6.5061\n",
      "Epoch 33/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 186.6517 - mae: 6.4529 - val_loss: 177.6331 - val_mae: 6.4640\n",
      "Epoch 34/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 186.4227 - mae: 6.4240 - val_loss: 177.3887 - val_mae: 6.4513\n",
      "Epoch 35/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 186.2831 - mae: 6.3930 - val_loss: 177.1993 - val_mae: 6.4422\n",
      "Epoch 36/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 185.9901 - mae: 6.4218 - val_loss: 177.3996 - val_mae: 6.4607\n",
      "Epoch 37/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 185.7591 - mae: 6.3953 - val_loss: 177.2037 - val_mae: 6.4514\n",
      "Epoch 38/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 185.5487 - mae: 6.4057 - val_loss: 177.1334 - val_mae: 6.4507\n",
      "Epoch 39/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 185.3247 - mae: 6.3836 - val_loss: 176.8497 - val_mae: 6.4361\n",
      "Epoch 40/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 185.0804 - mae: 6.3945 - val_loss: 176.8921 - val_mae: 6.4427\n",
      "Epoch 41/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 184.8991 - mae: 6.3767 - val_loss: 176.6860 - val_mae: 6.4335\n",
      "Epoch 42/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 184.7520 - mae: 6.3910 - val_loss: 176.5325 - val_mae: 6.4279\n",
      "Epoch 43/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 184.4277 - mae: 6.3691 - val_loss: 176.3660 - val_mae: 6.4215\n",
      "Epoch 44/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 184.2055 - mae: 6.3740 - val_loss: 176.2187 - val_mae: 6.4164\n",
      "Epoch 45/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 184.0065 - mae: 6.3621 - val_loss: 176.1891 - val_mae: 6.4179\n",
      "Epoch 46/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 183.8508 - mae: 6.3445 - val_loss: 175.9647 - val_mae: 6.4084\n",
      "Epoch 47/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 183.6852 - mae: 6.3731 - val_loss: 176.0252 - val_mae: 6.4147\n",
      "Epoch 48/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 183.4691 - mae: 6.3385 - val_loss: 175.7695 - val_mae: 6.4039\n",
      "Epoch 49/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 183.2233 - mae: 6.3527 - val_loss: 175.4794 - val_mae: 6.3925\n",
      "Epoch 50/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 182.9945 - mae: 6.3377 - val_loss: 175.5019 - val_mae: 6.3961\n",
      "Epoch 51/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 182.7630 - mae: 6.3404 - val_loss: 175.2549 - val_mae: 6.3873\n",
      "Epoch 52/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 182.5404 - mae: 6.3373 - val_loss: 175.3129 - val_mae: 6.3924\n",
      "Epoch 53/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 182.3885 - mae: 6.3314 - val_loss: 175.2487 - val_mae: 6.3920\n",
      "Epoch 54/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 182.1617 - mae: 6.3326 - val_loss: 175.1422 - val_mae: 6.3896\n",
      "Epoch 55/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 182.0224 - mae: 6.3295 - val_loss: 174.7853 - val_mae: 6.3774\n",
      "Epoch 56/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 181.8062 - mae: 6.3154 - val_loss: 174.4420 - val_mae: 6.3674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 181.6408 - mae: 6.3174 - val_loss: 174.6066 - val_mae: 6.3750\n",
      "Epoch 58/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 181.5670 - mae: 6.3301 - val_loss: 174.6501 - val_mae: 6.3791\n",
      "Epoch 59/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 181.2557 - mae: 6.3050 - val_loss: 174.3083 - val_mae: 6.3693\n",
      "Epoch 60/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 181.1541 - mae: 6.3141 - val_loss: 174.1568 - val_mae: 6.3669\n",
      "Epoch 61/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 180.9240 - mae: 6.3118 - val_loss: 174.2521 - val_mae: 6.3718\n",
      "Epoch 62/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 180.7231 - mae: 6.3075 - val_loss: 173.9138 - val_mae: 6.3644\n",
      "Epoch 63/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 180.4962 - mae: 6.3063 - val_loss: 174.1418 - val_mae: 6.3727\n",
      "Epoch 64/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 180.3551 - mae: 6.3027 - val_loss: 173.9837 - val_mae: 6.3702\n",
      "Epoch 65/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 180.1861 - mae: 6.3068 - val_loss: 173.8952 - val_mae: 6.3701\n",
      "Epoch 66/75\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 179.9833 - mae: 6.3021 - val_loss: 173.7108 - val_mae: 6.3678\n",
      "Epoch 67/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 179.8881 - mae: 6.2989 - val_loss: 173.7890 - val_mae: 6.3716\n",
      "Epoch 68/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 179.6904 - mae: 6.3014 - val_loss: 173.6999 - val_mae: 6.3716\n",
      "Epoch 69/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 179.5469 - mae: 6.2983 - val_loss: 173.4148 - val_mae: 6.3677\n",
      "Epoch 70/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 179.4319 - mae: 6.2999 - val_loss: 173.5058 - val_mae: 6.3713\n",
      "Epoch 71/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 179.2646 - mae: 6.2965 - val_loss: 173.4136 - val_mae: 6.3715\n",
      "Epoch 72/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 179.0735 - mae: 6.2996 - val_loss: 173.3824 - val_mae: 6.3729\n",
      "Epoch 73/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 178.8891 - mae: 6.3001 - val_loss: 173.3102 - val_mae: 6.3736\n",
      "Epoch 74/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 178.8145 - mae: 6.3018 - val_loss: 173.1727 - val_mae: 6.3735\n",
      "Epoch 75/75\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 178.6131 - mae: 6.2994 - val_loss: 173.1196 - val_mae: 6.3748\n",
      "264/264 [==============================] - 1s 2ms/step\n",
      "(8424,) (8424,) (8424,)\n",
      "8.10387905313758\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time # magic command in Jupyter Notebook that allows you to time how long it takes for a particular cell to run\n",
    "# NN model for AR(1) jointly for all hours\n",
    "inputs = keras.Input(2,) # define input layer - 2 independent variables\n",
    "# hidden = keras.layers.Dense(20, activation='relu')(inputs)\n",
    "outputs = keras.layers.Dense(1, activation='linear')(inputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics='mae')\n",
    "# specifies the optimizer to use during training (Adam), the loss function to use - MSE, and the evaluation metric to use - MAE\n",
    "\n",
    "# set out 20% data for validation\n",
    "perm = np.random.permutation(np.arange(Xt.shape[0]))\n",
    "VAL_DATA = 0.2\n",
    "trainsubset = perm[:int((1 - VAL_DATA) * len(perm))]\n",
    "valsubset = perm[int((1 - VAL_DATA) * len(perm)):]\n",
    "model.fit(Xt[trainsubset], Yt[trainsubset], epochs=75, validation_data=(Xt[valsubset], Yt[valsubset]), verbose=True, batch_size=256)\n",
    "# epochs parameter specifies the number of times to iterate over the entire training dataset during training\n",
    "# batch_size parameter specifies the number of samples to use for each update of the model weights\n",
    "NNpred = model.predict(Xf)[:, 0] # This passes the test inputs Xf through the trained model to generate predicted outputs\n",
    "print(NNpred.shape, Yf.shape, (NNpred - Yf).shape)\n",
    "print(np.mean(np.abs(NNpred - Yf))) #MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67db7eb-0cb8-4f4c-9fba-3b6d33558567",
   "metadata": {},
   "source": [
    "The second NN model\n",
    "------------------\n",
    "\n",
    "Let us add some complexity to the model.\n",
    "\n",
    "We add two additional layers:\n",
    " - a BatchNormalization layer\n",
    " - a hidden layer\n",
    " \n",
    "The batch normalization is useful if the input data is not normalized (e.g., we forecast prices using past prices with values ranging from -100 to 300 and load forecasts with values in tens of thousands) - so it is not really helpful here.\n",
    "\n",
    "Batch normalization normalizes the activations of the previous layer at each batch, i.e., it centers and scales the inputs to have zero mean and unit variance. This helps to prevent the activations from becoming too large or too small during training, which can slow down training or lead to numerical instability.\n",
    "\n",
    "The model might perform worse than the simpler one - but that just shows that the amount of input data is too low (the NN is able to model highly complex non-linear relations in the data, but the data we pass to the NN is very scarce).\n",
    "\n",
    "hidden = keras.layers.Dense(5, activation='elu')(bn) creates a dense layer with 5 units and the 'elu' activation function. This layer takes the output from the batch normalization layer and applies a linear transformation to it, followed by the activation function. The 'elu' activation function is the Exponential Linear Unit, which is a variant of the Rectified Linear Unit (ReLU) that allows for negative values. This can help to alleviate the vanishing gradient problem, which can occur when using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113aa1c7-d903-40ff-a111-8a55ccc3aadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2)                8         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 15        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29\n",
      "Trainable params: 25\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - 2s 5ms/step - loss: 2633.3181 - mae: 46.0509 - val_loss: 2447.7300 - val_mae: 44.6482\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 2565.2920 - mae: 45.3690 - val_loss: 2385.4546 - val_mae: 44.0269\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 2462.5605 - mae: 44.2536 - val_loss: 2290.4062 - val_mae: 42.9450\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 2345.7820 - mae: 42.8954 - val_loss: 2189.2278 - val_mae: 41.7237\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 2238.3894 - mae: 41.5787 - val_loss: 2095.7847 - val_mae: 40.5558\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 2145.2405 - mae: 40.4051 - val_loss: 2012.9957 - val_mae: 39.4950\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 2064.4055 - mae: 39.3583 - val_loss: 1938.9851 - val_mae: 38.5241\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1992.8860 - mae: 38.4172 - val_loss: 1872.4894 - val_mae: 37.6346\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1928.3901 - mae: 37.5519 - val_loss: 1811.3539 - val_mae: 36.8011\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1869.1399 - mae: 36.7442 - val_loss: 1754.9619 - val_mae: 36.0176\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1814.0219 - mae: 35.9757 - val_loss: 1701.9705 - val_mae: 35.2670\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1762.1809 - mae: 35.2401 - val_loss: 1652.0726 - val_mae: 34.5469\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1713.1008 - mae: 34.5303 - val_loss: 1604.5979 - val_mae: 33.8484\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1666.3376 - mae: 33.8429 - val_loss: 1559.3347 - val_mae: 33.1694\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1621.5483 - mae: 33.1720 - val_loss: 1515.9351 - val_mae: 32.5058\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1578.5508 - mae: 32.5153 - val_loss: 1474.2278 - val_mae: 31.8561\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1537.1595 - mae: 31.8696 - val_loss: 1433.9983 - val_mae: 31.2191\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1497.2524 - mae: 31.2366 - val_loss: 1395.1069 - val_mae: 30.5930\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1458.6984 - mae: 30.6161 - val_loss: 1357.6888 - val_mae: 29.9804\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 1421.3936 - mae: 30.0067 - val_loss: 1321.3481 - val_mae: 29.3765\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1385.2755 - mae: 29.4067 - val_loss: 1286.2201 - val_mae: 28.7829\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1350.3090 - mae: 28.8166 - val_loss: 1252.1771 - val_mae: 28.1984\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1316.3613 - mae: 28.2393 - val_loss: 1219.2010 - val_mae: 27.6210\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1283.4591 - mae: 27.6682 - val_loss: 1187.1746 - val_mae: 27.0509\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1251.5198 - mae: 27.1099 - val_loss: 1156.0398 - val_mae: 26.4887\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1220.5420 - mae: 26.5579 - val_loss: 1125.9075 - val_mae: 25.9369\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1190.5026 - mae: 26.0176 - val_loss: 1096.7562 - val_mae: 25.3967\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1161.3390 - mae: 25.4861 - val_loss: 1068.3601 - val_mae: 24.8627\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1133.0548 - mae: 24.9652 - val_loss: 1040.8590 - val_mae: 24.3371\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 1105.6207 - mae: 24.4524 - val_loss: 1014.2361 - val_mae: 23.8220\n",
      "264/264 [==============================] - 1s 2ms/step\n",
      "(8424,) (8424,) (8424,)\n",
      "30.460296155960357\n"
     ]
    }
   ],
   "source": [
    "# NN model for AR(1) jointly for all hours, improved\n",
    "inputs = keras.Input(2,) # define input layer - 2 independent variables\n",
    "\n",
    "bn = keras.layers.BatchNormalization()(inputs)\n",
    "\n",
    "hidden = keras.layers.Dense(5, activation='elu')(bn)\n",
    "\n",
    "outputs = keras.layers.Dense(1, activation='linear')(hidden)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics='mae')\n",
    "# set out 20% data for validation\n",
    "perm = np.random.permutation(np.arange(Xt.shape[0]))\n",
    "VAL_DATA = 0.2\n",
    "trainsubset = perm[:int((1 - VAL_DATA) * len(perm))]\n",
    "valsubset = perm[int((1 - VAL_DATA) * len(perm)):]\n",
    "model.fit(Xt[trainsubset], Yt[trainsubset], epochs=30, validation_data=(Xt[valsubset], Yt[valsubset]), verbose=True, batch_size=128)\n",
    "NNpred = model.predict(Xf)[:, 0]\n",
    "print(NNpred.shape, Yf.shape, (NNpred - Yf).shape)\n",
    "print(np.mean(np.abs(NNpred - Yf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f7dba-5ec3-4211-acc0-c68d5a41b4ce",
   "metadata": {},
   "source": [
    "The multi-output model\n",
    "----------------------\n",
    "\n",
    "Now, the model will have 24 outputs, one for each hour of the day. At the same time, the number of inputs needs to grow (we need to include the information about all hours of the day).\n",
    "\n",
    "Here, the scarcity of the model might be even more visible - as we still use only a fraction of the data we have available (and the network can handle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d519277-d0bb-4e8c-a5c5-4922729077ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 25)]              0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 25)               100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                1300      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24)                1224      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,624\n",
      "Trainable params: 2,574\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 10ms/step - loss: 2616.6648 - mae: 46.1386 - val_loss: 2228.7546 - val_mae: 41.3966\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2499.5115 - mae: 45.2394 - val_loss: 1747.6304 - val_mae: 36.7662\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2180.3889 - mae: 42.3144 - val_loss: 1184.6266 - val_mae: 30.6446\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1632.1185 - mae: 36.3669 - val_loss: 848.7514 - val_mae: 25.3981\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1138.2769 - mae: 29.4511 - val_loss: 813.8356 - val_mae: 23.3025\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 851.3679 - mae: 24.9327 - val_loss: 774.8193 - val_mae: 22.2743\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 720.0238 - mae: 22.4691 - val_loss: 699.7794 - val_mae: 20.9398\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 630.9312 - mae: 20.9803 - val_loss: 595.1208 - val_mae: 19.2847\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 522.2570 - mae: 18.8918 - val_loss: 489.6517 - val_mae: 17.4414\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 442.3426 - mae: 17.2918 - val_loss: 391.3430 - val_mae: 15.2592\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 355.7583 - mae: 15.1853 - val_loss: 298.3196 - val_mae: 12.9610\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 301.8453 - mae: 13.3667 - val_loss: 237.4923 - val_mae: 10.8719\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 227.1489 - mae: 11.3999 - val_loss: 179.0206 - val_mae: 8.9989\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 186.2850 - mae: 10.0337 - val_loss: 134.2853 - val_mae: 7.7317\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 180.7575 - mae: 9.2437 - val_loss: 119.5992 - val_mae: 6.8830\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 168.2913 - mae: 9.0040 - val_loss: 102.4505 - val_mae: 6.3109\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 151.8043 - mae: 8.3839 - val_loss: 92.8343 - val_mae: 5.9491\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 147.3914 - mae: 7.8913 - val_loss: 89.3108 - val_mae: 5.7618\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 141.2104 - mae: 7.5972 - val_loss: 88.5618 - val_mae: 5.5942\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 131.3010 - mae: 7.4245 - val_loss: 83.3177 - val_mae: 5.4702\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 128.6834 - mae: 7.1609 - val_loss: 82.8264 - val_mae: 5.3869\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 126.2782 - mae: 7.1604 - val_loss: 81.0347 - val_mae: 5.2979\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 135.3591 - mae: 7.2187 - val_loss: 80.1281 - val_mae: 5.2829\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 139.1346 - mae: 7.2378 - val_loss: 82.2932 - val_mae: 5.2549\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 123.6563 - mae: 7.1098 - val_loss: 77.6429 - val_mae: 5.1084\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 137.5034 - mae: 7.2079 - val_loss: 76.7285 - val_mae: 5.0994\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 124.5531 - mae: 6.8519 - val_loss: 76.2256 - val_mae: 5.0044\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 121.0445 - mae: 6.9350 - val_loss: 72.2184 - val_mae: 4.9457\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 117.0224 - mae: 6.6867 - val_loss: 71.0098 - val_mae: 4.9225\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 130.4767 - mae: 6.8988 - val_loss: 71.8703 - val_mae: 4.8965\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 122.4158 - mae: 6.8385 - val_loss: 71.7223 - val_mae: 4.8851\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 120.2003 - mae: 6.9385 - val_loss: 69.1549 - val_mae: 4.8199\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 124.0513 - mae: 6.9379 - val_loss: 68.9233 - val_mae: 4.8289\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 107.8652 - mae: 6.5465 - val_loss: 65.0345 - val_mae: 4.7334\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 126.8198 - mae: 6.8669 - val_loss: 66.4902 - val_mae: 4.8299\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 129.7738 - mae: 6.7435 - val_loss: 64.7160 - val_mae: 4.6560\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 120.0700 - mae: 6.7007 - val_loss: 61.6618 - val_mae: 4.5796\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 122.2441 - mae: 6.7226 - val_loss: 60.8104 - val_mae: 4.5715\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 110.8873 - mae: 6.4564 - val_loss: 59.9219 - val_mae: 4.5343\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 114.6397 - mae: 6.4196 - val_loss: 57.7044 - val_mae: 4.4769\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 112.0658 - mae: 6.5398 - val_loss: 56.8769 - val_mae: 4.4777\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 122.7495 - mae: 6.9411 - val_loss: 55.4079 - val_mae: 4.4735\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 113.3382 - mae: 6.4923 - val_loss: 56.2222 - val_mae: 4.5325\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 104.5763 - mae: 6.1852 - val_loss: 53.3353 - val_mae: 4.3557\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 104.5341 - mae: 6.0214 - val_loss: 52.5787 - val_mae: 4.3703\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 105.8772 - mae: 6.3856 - val_loss: 49.2966 - val_mae: 4.2990\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 110.8755 - mae: 6.2182 - val_loss: 50.2339 - val_mae: 4.2603\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 101.2652 - mae: 6.2003 - val_loss: 48.1626 - val_mae: 4.2203\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 101.6896 - mae: 6.2431 - val_loss: 47.3201 - val_mae: 4.2124\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 99.4425 - mae: 6.1348 - val_loss: 46.2365 - val_mae: 4.1775\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(351, 24) (351, 24) (351, 24)\n",
      "9.327819536486242\n"
     ]
    }
   ],
   "source": [
    "# NN model for AR(1) with 24 outputs\n",
    "inputs = keras.Input(25,) # define input layer - 1 independent variable for 24h + ones\n",
    "bn = keras.layers.BatchNormalization()(inputs) # Apply batch normalization to the input layer\n",
    "hidden = keras.layers.Dense(50, activation='elu')(bn) # Define a hidden layer with 50 nodes and an ELU activation function\n",
    "outputs = keras.layers.Dense(24, activation='linear')(hidden) # Define the output layer with 24 nodes, corresponding to the 24 hours of the day\n",
    "model = keras.Model(inputs=inputs, outputs=outputs) #Define the NN model by specifying the input and output layers\n",
    "print(model.summary())\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics='mae')\n",
    "# Compile the NN model using the Adam optimizer, mean squared error (MSE) loss function, and mean absolute error (MAE) metric\n",
    "# set out 20% data for validation\n",
    "\n",
    "VAL_DATA = 0.2\n",
    "Xt24 = np.hstack([Xt[:, 0].reshape(len(Xt) // 24, 24), Xt[::24, 1:]])\n",
    "perm = np.random.permutation(np.arange(Xt24.shape[0]))\n",
    "Xf24 = np.hstack([Xf[:, 0].reshape(len(Xf) // 24, 24), Xf[::24, 1:]])\n",
    "Y24 = Y.reshape(len(Y) // 24, 24)\n",
    "Yf24 = Yf.reshape(len(Yf) // 24, 24)\n",
    "trainsubset = perm[:int((1 - VAL_DATA) * len(perm))]\n",
    "valsubset = perm[int((1 - VAL_DATA) * len(perm)):]\n",
    "model.fit(Xt24[trainsubset], Y24[trainsubset], epochs=50, validation_data=(Xt24[valsubset], Y24[valsubset]), verbose=True, batch_size=16)\n",
    "# Train the NN model using the training dataset and the validation dataset for 50 epochs, a batch size of 16, and verbose output\n",
    "NNpred = model.predict(Xf24)[:, :]\n",
    "# Use the trained NN model to predict the 24-hour outputs for the test dataset Xf24, and compute the mean absolute error (MAE) between the predicted outputs NNpred and the true outputs Yf24\n",
    "print(NNpred.shape, Yf24.shape, (NNpred - Yf24).shape)\n",
    "print(np.mean(np.abs(NNpred - Yf24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4afac-f5e4-4a1b-8588-bf8504c14ead",
   "metadata": {},
   "source": [
    "Hyper-parameter optimization\n",
    "----------------------------\n",
    "\n",
    "In the previous examples, the parameters such as number of neurons were set at some level. However, the neural networks have a lot of hyper-parameters (especially the larger ones), for example, we can even have a hyper-parameter that decides whether to include a data processing step or to include a given input variable in the model.\n",
    "\n",
    "Hyper-parameters are, by definition, parameters of the model that are not \"trained\" when training the model (the \"trainable parameters\" are the weights in the network, correponding to the beta coefficients in linear regression). We need to set the hyper-parameters prior to training the neural network, their values are typically determined in the separate optimization study.\n",
    "\n",
    "Here, we will optimize the number of neurons using optuna package. Note, that we need to set out some data to evaluate the hyper-parameter sets (and this set should be separate from the out-of-sample test that will be used to evaluate the model after choosing the hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa77b39-d5b9-4e09-aad7-266df328e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 25) (60, 25) (670, 24) (60, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-10 16:20:59,054]\u001b[0m A new study created in RDB with name: study\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in RDB with name: study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia-\\anaconda3\\lib\\site-packages\\optuna\\progress_bar.py:56: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0254865554864eda88fa415016e3a9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 0 finished with value: 3.1743979552586876 and parameters: {'neurons': 51, 'activation': 'relu'}. Best is trial 0 with value: 3.1743979552586876.\n",
      "\u001b[32m[I 2023-05-10 16:21:04,967]\u001b[0m Trial 0 finished with value: 3.1743979552586876 and parameters: {'neurons': 51, 'activation': 'relu'}. Best is trial 0 with value: 3.1743979552586876.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 1 finished with value: 5.852950577947829 and parameters: {'neurons': 12, 'activation': 'linear'}. Best is trial 0 with value: 3.1743979552586876.\n",
      "\u001b[32m[I 2023-05-10 16:21:10,878]\u001b[0m Trial 1 finished with value: 5.852950577947829 and parameters: {'neurons': 12, 'activation': 'linear'}. Best is trial 0 with value: 3.1743979552586876.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 2 finished with value: 3.2065070124732125 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 0 with value: 3.1743979552586876.\n",
      "\u001b[32m[I 2023-05-10 16:21:15,316]\u001b[0m Trial 2 finished with value: 3.2065070124732125 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 0 with value: 3.1743979552586876.\u001b[0m\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000275E91BAEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 3 finished with value: 3.118101844522688 and parameters: {'neurons': 35, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:22,317]\u001b[0m Trial 3 finished with value: 3.118101844522688 and parameters: {'neurons': 35, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "WARNING:tensorflow:6 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000275EAB26820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 4 finished with value: 4.917020830419328 and parameters: {'neurons': 8, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:28,293]\u001b[0m Trial 4 finished with value: 4.917020830419328 and parameters: {'neurons': 8, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 5 finished with value: 6.824574360052745 and parameters: {'neurons': 32, 'activation': 'sigmoid'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:34,154]\u001b[0m Trial 5 finished with value: 6.824574360052745 and parameters: {'neurons': 32, 'activation': 'sigmoid'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 6 finished with value: 5.382674211343129 and parameters: {'neurons': 93, 'activation': 'sigmoid'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:39,494]\u001b[0m Trial 6 finished with value: 5.382674211343129 and parameters: {'neurons': 93, 'activation': 'sigmoid'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 7 finished with value: 4.867454753981696 and parameters: {'neurons': 29, 'activation': 'linear'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:43,289]\u001b[0m Trial 7 finished with value: 4.867454753981696 and parameters: {'neurons': 29, 'activation': 'linear'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 8 finished with value: 3.311587907526228 and parameters: {'neurons': 84, 'activation': 'elu'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:48,305]\u001b[0m Trial 8 finished with value: 3.311587907526228 and parameters: {'neurons': 84, 'activation': 'elu'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 9 finished with value: 3.49131596575843 and parameters: {'neurons': 28, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:52,284]\u001b[0m Trial 9 finished with value: 3.49131596575843 and parameters: {'neurons': 28, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 10 finished with value: 3.965778861999512 and parameters: {'neurons': 64, 'activation': 'elu'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:21:56,117]\u001b[0m Trial 10 finished with value: 3.965778861999512 and parameters: {'neurons': 64, 'activation': 'elu'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 11 finished with value: 3.2301376547813416 and parameters: {'neurons': 54, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\n",
      "\u001b[32m[I 2023-05-10 16:22:00,822]\u001b[0m Trial 11 finished with value: 3.2301376547813416 and parameters: {'neurons': 54, 'activation': 'relu'}. Best is trial 3 with value: 3.118101844522688.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 12 finished with value: 3.0218259024620053 and parameters: {'neurons': 47, 'activation': 'relu'}. Best is trial 12 with value: 3.0218259024620053.\n",
      "\u001b[32m[I 2023-05-10 16:22:06,266]\u001b[0m Trial 12 finished with value: 3.0218259024620053 and parameters: {'neurons': 47, 'activation': 'relu'}. Best is trial 12 with value: 3.0218259024620053.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 13 finished with value: 3.178203739696079 and parameters: {'neurons': 43, 'activation': 'relu'}. Best is trial 12 with value: 3.0218259024620053.\n",
      "\u001b[32m[I 2023-05-10 16:22:11,202]\u001b[0m Trial 13 finished with value: 3.178203739696079 and parameters: {'neurons': 43, 'activation': 'relu'}. Best is trial 12 with value: 3.0218259024620053.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 14 finished with value: 2.965909490426381 and parameters: {'neurons': 73, 'activation': 'relu'}. Best is trial 14 with value: 2.965909490426381.\n",
      "\u001b[32m[I 2023-05-10 16:22:14,808]\u001b[0m Trial 14 finished with value: 2.965909490426381 and parameters: {'neurons': 73, 'activation': 'relu'}. Best is trial 14 with value: 2.965909490426381.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 15 finished with value: 3.0943299465179446 and parameters: {'neurons': 74, 'activation': 'relu'}. Best is trial 14 with value: 2.965909490426381.\n",
      "\u001b[32m[I 2023-05-10 16:22:20,233]\u001b[0m Trial 15 finished with value: 3.0943299465179446 and parameters: {'neurons': 74, 'activation': 'relu'}. Best is trial 14 with value: 2.965909490426381.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 16 finished with value: 3.6365220805274117 and parameters: {'neurons': 67, 'activation': 'elu'}. Best is trial 14 with value: 2.965909490426381.\n",
      "\u001b[32m[I 2023-05-10 16:22:24,805]\u001b[0m Trial 16 finished with value: 3.6365220805274117 and parameters: {'neurons': 67, 'activation': 'elu'}. Best is trial 14 with value: 2.965909490426381.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 17 finished with value: 3.330535571681129 and parameters: {'neurons': 78, 'activation': 'linear'}. Best is trial 14 with value: 2.965909490426381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-10 16:22:29,505]\u001b[0m Trial 17 finished with value: 3.330535571681129 and parameters: {'neurons': 78, 'activation': 'linear'}. Best is trial 14 with value: 2.965909490426381.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 18 finished with value: 5.359215482976702 and parameters: {'neurons': 58, 'activation': 'sigmoid'}. Best is trial 14 with value: 2.965909490426381.\n",
      "\u001b[32m[I 2023-05-10 16:22:34,859]\u001b[0m Trial 18 finished with value: 5.359215482976702 and parameters: {'neurons': 58, 'activation': 'sigmoid'}. Best is trial 14 with value: 2.965909490426381.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 19 finished with value: 2.8979457795884875 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 19 with value: 2.8979457795884875.\n",
      "\u001b[32m[I 2023-05-10 16:22:40,272]\u001b[0m Trial 19 finished with value: 2.8979457795884875 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 19 with value: 2.8979457795884875.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 20 finished with value: 2.7048438454733956 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:22:44,220]\u001b[0m Trial 20 finished with value: 2.7048438454733956 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 21 finished with value: 2.8799807797008095 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:22:48,368]\u001b[0m Trial 21 finished with value: 2.8799807797008095 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 22 finished with value: 2.877831816726261 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:22:53,479]\u001b[0m Trial 22 finished with value: 2.877831816726261 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 23 finished with value: 2.8333988903363547 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:22:57,579]\u001b[0m Trial 23 finished with value: 2.8333988903363547 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 24 finished with value: 2.7558516169654 and parameters: {'neurons': 88, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:03,168]\u001b[0m Trial 24 finished with value: 2.7558516169654 and parameters: {'neurons': 88, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 25 finished with value: 3.0678547656271196 and parameters: {'neurons': 85, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:07,339]\u001b[0m Trial 25 finished with value: 3.0678547656271196 and parameters: {'neurons': 85, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 26 finished with value: 3.8477920309172733 and parameters: {'neurons': 89, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:11,981]\u001b[0m Trial 26 finished with value: 3.8477920309172733 and parameters: {'neurons': 89, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 27 finished with value: 5.3989490417904324 and parameters: {'neurons': 79, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:17,334]\u001b[0m Trial 27 finished with value: 5.3989490417904324 and parameters: {'neurons': 79, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 28 finished with value: 3.179435074276394 and parameters: {'neurons': 92, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:21,916]\u001b[0m Trial 28 finished with value: 3.179435074276394 and parameters: {'neurons': 92, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 29 finished with value: 2.7215709203614127 and parameters: {'neurons': 86, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:25,547]\u001b[0m Trial 29 finished with value: 2.7215709203614127 and parameters: {'neurons': 86, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 30 finished with value: 2.976496375931634 and parameters: {'neurons': 68, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:30,326]\u001b[0m Trial 30 finished with value: 2.976496375931634 and parameters: {'neurons': 68, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 31 finished with value: 2.7715170168346828 and parameters: {'neurons': 84, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:35,834]\u001b[0m Trial 31 finished with value: 2.7715170168346828 and parameters: {'neurons': 84, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 32 finished with value: 2.9661852913962474 and parameters: {'neurons': 85, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:41,453]\u001b[0m Trial 32 finished with value: 2.9661852913962474 and parameters: {'neurons': 85, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 33 finished with value: 2.9062303727467858 and parameters: {'neurons': 80, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:46,976]\u001b[0m Trial 33 finished with value: 2.9062303727467858 and parameters: {'neurons': 80, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 34 finished with value: 3.049797819561429 and parameters: {'neurons': 93, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:50,864]\u001b[0m Trial 34 finished with value: 3.049797819561429 and parameters: {'neurons': 93, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 35 finished with value: 2.9459331021838717 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:23:56,293]\u001b[0m Trial 35 finished with value: 2.9459331021838717 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 36 finished with value: 3.578504219796922 and parameters: {'neurons': 72, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:01,368]\u001b[0m Trial 36 finished with value: 3.578504219796922 and parameters: {'neurons': 72, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 37 finished with value: 3.066658284028371 and parameters: {'neurons': 61, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:05,812]\u001b[0m Trial 37 finished with value: 3.066658284028371 and parameters: {'neurons': 61, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 38 finished with value: 19.750243296570247 and parameters: {'neurons': 14, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:09,232]\u001b[0m Trial 38 finished with value: 19.750243296570247 and parameters: {'neurons': 14, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 39 finished with value: 3.01310589032703 and parameters: {'neurons': 95, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:14,395]\u001b[0m Trial 39 finished with value: 3.01310589032703 and parameters: {'neurons': 95, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 40 finished with value: 3.0596684892442494 and parameters: {'neurons': 86, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:20,207]\u001b[0m Trial 40 finished with value: 3.0596684892442494 and parameters: {'neurons': 86, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 41 finished with value: 2.7226167065832354 and parameters: {'neurons': 96, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:27,193]\u001b[0m Trial 41 finished with value: 2.7226167065832354 and parameters: {'neurons': 96, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 42 finished with value: 2.855204133934445 and parameters: {'neurons': 96, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:31,999]\u001b[0m Trial 42 finished with value: 2.855204133934445 and parameters: {'neurons': 96, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 43 finished with value: 2.821259500556522 and parameters: {'neurons': 81, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:35,839]\u001b[0m Trial 43 finished with value: 2.821259500556522 and parameters: {'neurons': 81, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 44 finished with value: 2.9711571465598214 and parameters: {'neurons': 88, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:40,797]\u001b[0m Trial 44 finished with value: 2.9711571465598214 and parameters: {'neurons': 88, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 45 finished with value: 3.5866648175981313 and parameters: {'neurons': 96, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:45,685]\u001b[0m Trial 45 finished with value: 3.5866648175981313 and parameters: {'neurons': 96, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 46 finished with value: 2.9590209541850623 and parameters: {'neurons': 76, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:50,260]\u001b[0m Trial 46 finished with value: 2.9590209541850623 and parameters: {'neurons': 76, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 47 finished with value: 3.525614790492588 and parameters: {'neurons': 83, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:53,700]\u001b[0m Trial 47 finished with value: 3.525614790492588 and parameters: {'neurons': 83, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 48 finished with value: 3.226315069039663 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:24:57,722]\u001b[0m Trial 48 finished with value: 3.226315069039663 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 49 finished with value: 5.445029890272353 and parameters: {'neurons': 95, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:02,119]\u001b[0m Trial 49 finished with value: 5.445029890272353 and parameters: {'neurons': 95, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 50 finished with value: 5.4350565480126285 and parameters: {'neurons': 17, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:07,410]\u001b[0m Trial 50 finished with value: 5.4350565480126285 and parameters: {'neurons': 17, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 51 finished with value: 2.9224269838333132 and parameters: {'neurons': 83, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:12,214]\u001b[0m Trial 51 finished with value: 2.9224269838333132 and parameters: {'neurons': 83, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 52 finished with value: 3.1321467000643413 and parameters: {'neurons': 81, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:17,361]\u001b[0m Trial 52 finished with value: 3.1321467000643413 and parameters: {'neurons': 81, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 53 finished with value: 3.147547435389625 and parameters: {'neurons': 70, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:22,362]\u001b[0m Trial 53 finished with value: 3.147547435389625 and parameters: {'neurons': 70, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 54 finished with value: 2.9155611417028644 and parameters: {'neurons': 76, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:26,144]\u001b[0m Trial 54 finished with value: 2.9155611417028644 and parameters: {'neurons': 76, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 55 finished with value: 3.2878626676665412 and parameters: {'neurons': 88, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:30,070]\u001b[0m Trial 55 finished with value: 3.2878626676665412 and parameters: {'neurons': 88, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 56 finished with value: 2.799680526892345 and parameters: {'neurons': 97, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:33,904]\u001b[0m Trial 56 finished with value: 2.799680526892345 and parameters: {'neurons': 97, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 57 finished with value: 3.3913717914687265 and parameters: {'neurons': 38, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:37,659]\u001b[0m Trial 57 finished with value: 3.3913717914687265 and parameters: {'neurons': 38, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 58 finished with value: 2.967602454503377 and parameters: {'neurons': 97, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:43,135]\u001b[0m Trial 58 finished with value: 2.967602454503377 and parameters: {'neurons': 97, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 59 finished with value: 2.8995414358245 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:47,507]\u001b[0m Trial 59 finished with value: 2.8995414358245 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 60 finished with value: 3.16297946744495 and parameters: {'neurons': 98, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:52,923]\u001b[0m Trial 60 finished with value: 3.16297946744495 and parameters: {'neurons': 98, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 61 finished with value: 3.604674173037211 and parameters: {'neurons': 93, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:25:57,266]\u001b[0m Trial 61 finished with value: 3.604674173037211 and parameters: {'neurons': 93, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 62 finished with value: 2.968800450854831 and parameters: {'neurons': 87, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:02,597]\u001b[0m Trial 62 finished with value: 2.968800450854831 and parameters: {'neurons': 87, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 63 finished with value: 2.722164461506738 and parameters: {'neurons': 82, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:06,149]\u001b[0m Trial 63 finished with value: 2.722164461506738 and parameters: {'neurons': 82, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 64 finished with value: 3.0925740091005967 and parameters: {'neurons': 53, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:09,979]\u001b[0m Trial 64 finished with value: 3.0925740091005967 and parameters: {'neurons': 53, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 65 finished with value: 3.0286648314793903 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:15,166]\u001b[0m Trial 65 finished with value: 3.0286648314793903 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 66 finished with value: 5.4458229647212555 and parameters: {'neurons': 84, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:20,502]\u001b[0m Trial 66 finished with value: 5.4458229647212555 and parameters: {'neurons': 84, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 67 finished with value: 2.7908150614632503 and parameters: {'neurons': 99, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:25,563]\u001b[0m Trial 67 finished with value: 2.7908150614632503 and parameters: {'neurons': 99, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 68 finished with value: 3.661446756839752 and parameters: {'neurons': 100, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:30,333]\u001b[0m Trial 68 finished with value: 3.661446756839752 and parameters: {'neurons': 100, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 69 finished with value: 2.7873363145192465 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:34,674]\u001b[0m Trial 69 finished with value: 2.7873363145192465 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 70 finished with value: 3.036037816471523 and parameters: {'neurons': 77, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:39,369]\u001b[0m Trial 70 finished with value: 3.036037816471523 and parameters: {'neurons': 77, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 71 finished with value: 2.827952542728848 and parameters: {'neurons': 91, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:42,958]\u001b[0m Trial 71 finished with value: 2.827952542728848 and parameters: {'neurons': 91, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 72 finished with value: 2.9001467135217456 and parameters: {'neurons': 88, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:47,856]\u001b[0m Trial 72 finished with value: 2.9001467135217456 and parameters: {'neurons': 88, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 73 finished with value: 3.211901350445218 and parameters: {'neurons': 94, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:53,126]\u001b[0m Trial 73 finished with value: 3.211901350445218 and parameters: {'neurons': 94, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 74 finished with value: 2.9297119697994654 and parameters: {'neurons': 86, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:26:56,555]\u001b[0m Trial 74 finished with value: 2.9297119697994654 and parameters: {'neurons': 86, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 75 finished with value: 3.0704848905139506 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:01,205]\u001b[0m Trial 75 finished with value: 3.0704848905139506 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 76 finished with value: 3.0488303708500326 and parameters: {'neurons': 98, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:06,217]\u001b[0m Trial 76 finished with value: 3.0488303708500326 and parameters: {'neurons': 98, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 77 finished with value: 2.8634950313568117 and parameters: {'neurons': 48, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:11,502]\u001b[0m Trial 77 finished with value: 2.8634950313568117 and parameters: {'neurons': 48, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 78 finished with value: 3.1203736598756575 and parameters: {'neurons': 82, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:15,772]\u001b[0m Trial 78 finished with value: 3.1203736598756575 and parameters: {'neurons': 82, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 79 finished with value: 2.8693022814326814 and parameters: {'neurons': 79, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:20,010]\u001b[0m Trial 79 finished with value: 2.8693022814326814 and parameters: {'neurons': 79, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 80 finished with value: 5.395171118948195 and parameters: {'neurons': 95, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:24,536]\u001b[0m Trial 80 finished with value: 5.395171118948195 and parameters: {'neurons': 95, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 81 finished with value: 3.090227739599016 and parameters: {'neurons': 98, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:28,694]\u001b[0m Trial 81 finished with value: 3.090227739599016 and parameters: {'neurons': 98, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 82 finished with value: 2.873054886341095 and parameters: {'neurons': 94, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:32,231]\u001b[0m Trial 82 finished with value: 2.873054886341095 and parameters: {'neurons': 94, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 83 finished with value: 2.8344791110886467 and parameters: {'neurons': 98, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:35,873]\u001b[0m Trial 83 finished with value: 2.8344791110886467 and parameters: {'neurons': 98, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 84 finished with value: 2.8984200172954138 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:40,599]\u001b[0m Trial 84 finished with value: 2.8984200172954138 and parameters: {'neurons': 90, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 85 finished with value: 2.786549697558085 and parameters: {'neurons': 85, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:45,760]\u001b[0m Trial 85 finished with value: 2.786549697558085 and parameters: {'neurons': 85, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 86 finished with value: 3.063533712916904 and parameters: {'neurons': 74, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:50,979]\u001b[0m Trial 86 finished with value: 3.063533712916904 and parameters: {'neurons': 74, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 87 finished with value: 3.828050508923001 and parameters: {'neurons': 86, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:27:56,425]\u001b[0m Trial 87 finished with value: 3.828050508923001 and parameters: {'neurons': 86, 'activation': 'elu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 88 finished with value: 3.7867082168261206 and parameters: {'neurons': 21, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:01,166]\u001b[0m Trial 88 finished with value: 3.7867082168261206 and parameters: {'neurons': 21, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 89 finished with value: 2.9146335502730474 and parameters: {'neurons': 84, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:05,929]\u001b[0m Trial 89 finished with value: 2.9146335502730474 and parameters: {'neurons': 84, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Trial 90 finished with value: 2.8944723677635196 and parameters: {'neurons': 65, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:11,647]\u001b[0m Trial 90 finished with value: 2.8944723677635196 and parameters: {'neurons': 65, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 91 finished with value: 2.8866268221537275 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:16,607]\u001b[0m Trial 91 finished with value: 2.8866268221537275 and parameters: {'neurons': 100, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 92 finished with value: 3.148333682854971 and parameters: {'neurons': 96, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:22,684]\u001b[0m Trial 92 finished with value: 3.148333682854971 and parameters: {'neurons': 96, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Trial 93 finished with value: 2.8457470666037668 and parameters: {'neurons': 89, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:27,979]\u001b[0m Trial 93 finished with value: 2.8457470666037668 and parameters: {'neurons': 89, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 94 finished with value: 2.805747757911682 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:33,128]\u001b[0m Trial 94 finished with value: 2.805747757911682 and parameters: {'neurons': 92, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Trial 95 finished with value: 2.8538107790417144 and parameters: {'neurons': 82, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:38,675]\u001b[0m Trial 95 finished with value: 2.8538107790417144 and parameters: {'neurons': 82, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 96 finished with value: 3.0128010582394067 and parameters: {'neurons': 57, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:44,597]\u001b[0m Trial 96 finished with value: 3.0128010582394067 and parameters: {'neurons': 57, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 97 finished with value: 3.031409574402703 and parameters: {'neurons': 87, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:50,019]\u001b[0m Trial 97 finished with value: 3.031409574402703 and parameters: {'neurons': 87, 'activation': 'linear'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 98 finished with value: 3.0058259024620053 and parameters: {'neurons': 94, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:28:55,148]\u001b[0m Trial 98 finished with value: 3.0058259024620053 and parameters: {'neurons': 94, 'activation': 'relu'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Trial 99 finished with value: 5.383954962518479 and parameters: {'neurons': 78, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\n",
      "\u001b[32m[I 2023-05-10 16:29:01,142]\u001b[0m Trial 99 finished with value: 5.383954962518479 and parameters: {'neurons': 78, 'activation': 'sigmoid'}. Best is trial 20 with value: 2.7048438454733956.\u001b[0m\n",
      "{'activation': 'relu', 'neurons': 100}\n",
      "Model: \"model_103\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_104 (InputLayer)      [(None, 25)]              0         \n",
      "                                                                 \n",
      " batch_normalization_102 (Ba  (None, 25)               100       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 100)               2600      \n",
      "                                                                 \n",
      " dense_206 (Dense)           (None, 24)                2424      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,124\n",
      "Trainable params: 5,074\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "37/37 [==============================] - 2s 10ms/step - loss: 2620.8069 - mae: 45.5228 - val_loss: 1411.1556 - val_mae: 34.5011\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2259.1536 - mae: 42.0079 - val_loss: 686.6703 - val_mae: 23.3884\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1551.8483 - mae: 34.1175 - val_loss: 371.0376 - val_mae: 16.2969\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 972.6638 - mae: 25.6831 - val_loss: 427.4971 - val_mae: 16.7149\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 765.1603 - mae: 22.3653 - val_loss: 436.7204 - val_mae: 17.1875\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 749.8033 - mae: 22.2554 - val_loss: 438.6616 - val_mae: 17.9707\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 709.7141 - mae: 22.0489 - val_loss: 433.3550 - val_mae: 17.9588\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 623.2224 - mae: 20.5946 - val_loss: 414.9409 - val_mae: 17.6161\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 583.5446 - mae: 19.8288 - val_loss: 369.2523 - val_mae: 16.6249\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 509.9044 - mae: 18.1432 - val_loss: 313.6562 - val_mae: 15.2615\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 449.7051 - mae: 17.0230 - val_loss: 255.6837 - val_mae: 13.7447\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 354.7755 - mae: 14.7463 - val_loss: 184.9956 - val_mae: 11.4493\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 295.5869 - mae: 13.0538 - val_loss: 126.3567 - val_mae: 9.0565\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 226.8252 - mae: 10.7190 - val_loss: 82.9555 - val_mae: 6.8930\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 193.2484 - mae: 9.1038 - val_loss: 67.7596 - val_mae: 6.1348\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 176.4568 - mae: 8.4200 - val_loss: 57.2399 - val_mae: 5.4975\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 165.0610 - mae: 8.1090 - val_loss: 52.2068 - val_mae: 5.2306\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 154.6894 - mae: 7.4855 - val_loss: 49.1890 - val_mae: 5.1060\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 150.3092 - mae: 7.4042 - val_loss: 41.3938 - val_mae: 4.5193\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 153.0076 - mae: 7.2330 - val_loss: 40.0033 - val_mae: 4.5100\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 151.1585 - mae: 7.1822 - val_loss: 36.3981 - val_mae: 4.2825\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 149.3419 - mae: 6.9622 - val_loss: 34.6765 - val_mae: 4.1815\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 127.4678 - mae: 6.5157 - val_loss: 32.0331 - val_mae: 4.0231\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 139.2045 - mae: 6.5139 - val_loss: 31.1843 - val_mae: 3.9138\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 110.1191 - mae: 6.3420 - val_loss: 29.7253 - val_mae: 3.8932\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 147.9112 - mae: 6.9451 - val_loss: 32.1510 - val_mae: 4.0335\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 126.3822 - mae: 6.3554 - val_loss: 28.6418 - val_mae: 3.8118\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 107.1968 - mae: 5.9112 - val_loss: 28.1187 - val_mae: 3.7699\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 124.9948 - mae: 6.2307 - val_loss: 25.6454 - val_mae: 3.5932\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 121.9184 - mae: 6.2711 - val_loss: 25.9076 - val_mae: 3.6212\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 110.3447 - mae: 6.1285 - val_loss: 25.2670 - val_mae: 3.5704\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 133.8290 - mae: 6.1366 - val_loss: 25.6804 - val_mae: 3.5965\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 135.3899 - mae: 6.5450 - val_loss: 28.0264 - val_mae: 3.8072\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 106.3376 - mae: 5.8300 - val_loss: 24.8641 - val_mae: 3.5629\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 112.8546 - mae: 6.0298 - val_loss: 24.1451 - val_mae: 3.5258\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 121.8129 - mae: 6.2714 - val_loss: 22.9467 - val_mae: 3.4082\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 104.4777 - mae: 5.7934 - val_loss: 22.0464 - val_mae: 3.3519\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 108.6310 - mae: 5.9362 - val_loss: 23.9515 - val_mae: 3.5131\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 113.7897 - mae: 5.9341 - val_loss: 26.0515 - val_mae: 3.6869\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 130.2288 - mae: 6.1734 - val_loss: 22.2064 - val_mae: 3.3505\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 121.5789 - mae: 6.3474 - val_loss: 26.2141 - val_mae: 3.7028\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 102.4369 - mae: 5.8179 - val_loss: 21.2021 - val_mae: 3.2991\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 104.8643 - mae: 5.9604 - val_loss: 23.8815 - val_mae: 3.5355\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 122.8007 - mae: 6.1828 - val_loss: 23.1346 - val_mae: 3.4550\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 111.7537 - mae: 5.8320 - val_loss: 22.3236 - val_mae: 3.3889\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 119.3530 - mae: 5.9892 - val_loss: 21.5177 - val_mae: 3.3240\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 104.3480 - mae: 5.8897 - val_loss: 21.3999 - val_mae: 3.3407\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 105.9175 - mae: 5.8796 - val_loss: 25.3952 - val_mae: 3.6741\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 115.5226 - mae: 6.2010 - val_loss: 21.0767 - val_mae: 3.2920\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 124.4908 - mae: 6.0968 - val_loss: 20.7967 - val_mae: 3.2887\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "(351, 24) (351, 24) (351, 24)\n",
      "9.347363136831286\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import logging\n",
    "import sys\n",
    "Xt24 = np.hstack([Xt[:, 0].reshape(len(Xt) // 24, 24), Xt[::24, 1:]])\n",
    "Xf24 = np.hstack([Xf[:, 0].reshape(len(Xf) // 24, 24), Xf[::24, 1:]])\n",
    "Y24 = Y.reshape(len(Y) // 24, 24)\n",
    "Y24 = Y24[:len(Xt24)]\n",
    "Yf24 = Yf.reshape(len(Yf) // 24, 24)\n",
    "\n",
    "# hyper-parameter optimization step\n",
    "# we will use the part of the training data - with the last 60 days being left out for the evaluation\n",
    "Xt_opti = Xt24[:-60]\n",
    "Xf_opti = Xt24[-60:]\n",
    "Yt_opti = Y24[:-60]\n",
    "Yf_opti = Y24[-60:]\n",
    "print(Xt_opti.shape, Xf_opti.shape, Yt_opti.shape, Yf_opti.shape)\n",
    "\n",
    "def objective(trial):\n",
    "    # function for the hyper-parameter optimization of the neural network model. The function takes in a trial object from the Optuna \n",
    "    #library, which contains the parameters being optimized. The function defines a neural network with one hidden layer and the number \n",
    "    #of hidden neurons and the activation function of the hidden layer are optimized. The function returns the mean absolute error between the predicted and actual values on the validation set.\n",
    "    \n",
    "    # Build a NN that has one hidden layer and optimize the number of hiden neurons and the hidden activation\n",
    "    neurons = trial.suggest_int('neurons', 3, 100)\n",
    "    activation = trial.suggest_categorical('activation', ['elu', 'relu', 'linear', 'sigmoid'])\n",
    "    # the rest of the parameters will be fixed\n",
    "    inputs = keras.Input(25,) # define input layer - 1 independent variable for 24h + ones\n",
    "    bn = keras.layers.BatchNormalization()(inputs)\n",
    "    hidden = keras.layers.Dense(neurons, activation=activation)(bn)\n",
    "    outputs = keras.layers.Dense(24, activation='linear')(hidden)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics='mae')\n",
    "    # note we don't have the validation here - this is only to simplify, it can and should be used\n",
    "    model.fit(Xt_opti, Yt_opti, epochs=50, verbose=False, batch_size=16)\n",
    "    NNpred = model.predict(Xf_opti)[:, :]\n",
    "    return np.mean(np.abs(NNpred - Yf_opti))\n",
    "\n",
    "optuna.logging.get_logger('optuna').addHandler(logging.StreamHandler(sys.stdout)) # sets up logging to output the results of the optimization to the console\n",
    "study_name = 'study'\n",
    "storage_name = 'sqlite:///study.sqlite'\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=False) # creates a new study object with the specified name and database storage location\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True) # runs the optimization process on the defined objective function, with 100 trials to search the hyperparameter space and a progress bar to display the status of the optimization\n",
    "\n",
    "best_params = study.best_params #  retrieves the best set of hyperparameters found by the optimization process\n",
    "print(best_params)\n",
    "\n",
    "# NN model for AR(1) with 24 outputs with hyper-parameter optimization -- using the optimized parameters\n",
    "inputs = keras.Input(25,) # define input layer - 1 independent variable for 24h + ones\n",
    "bn = keras.layers.BatchNormalization()(inputs)\n",
    "hidden = keras.layers.Dense(best_params['neurons'], activation=best_params['activation'])(bn)\n",
    "outputs = keras.layers.Dense(24, activation='linear')(hidden)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics='mae')\n",
    "# set out 20% data for validation\n",
    "\n",
    "VAL_DATA = 0.2\n",
    "perm = np.random.permutation(np.arange(Xt24.shape[0]))\n",
    "\n",
    "trainsubset = perm[:int((1 - VAL_DATA) * len(perm))]\n",
    "valsubset = perm[int((1 - VAL_DATA) * len(perm)):]\n",
    "model.fit(Xt24[trainsubset], Y24[trainsubset], epochs=50, validation_data=(Xt24[valsubset], Y24[valsubset]), verbose=True, batch_size=16)\n",
    "NNpred = model.predict(Xf24)[:, :]\n",
    "print(NNpred.shape, Yf24.shape, (NNpred - Yf24).shape)\n",
    "print(np.mean(np.abs(NNpred - Yf24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1b0369-4b91-4545-8280-249167a953f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.347363136831286\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(NNpred - Yf24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef20efd2-8a28-4f55-9002-2eaa7482b11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'neurons': 100}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17fcf952-2662-4e7b-9992-d4225dc28a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_activation</th>\n",
       "      <th>params_neurons</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.174398</td>\n",
       "      <td>2023-05-10 16:20:59.149699</td>\n",
       "      <td>2023-05-10 16:21:04.908337</td>\n",
       "      <td>0 days 00:00:05.758638</td>\n",
       "      <td>relu</td>\n",
       "      <td>51</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.852951</td>\n",
       "      <td>2023-05-10 16:21:05.009577</td>\n",
       "      <td>2023-05-10 16:21:10.817795</td>\n",
       "      <td>0 days 00:00:05.808218</td>\n",
       "      <td>linear</td>\n",
       "      <td>12</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.206507</td>\n",
       "      <td>2023-05-10 16:21:10.943480</td>\n",
       "      <td>2023-05-10 16:21:15.280943</td>\n",
       "      <td>0 days 00:00:04.337463</td>\n",
       "      <td>relu</td>\n",
       "      <td>92</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.118102</td>\n",
       "      <td>2023-05-10 16:21:15.357263</td>\n",
       "      <td>2023-05-10 16:21:22.269394</td>\n",
       "      <td>0 days 00:00:06.912131</td>\n",
       "      <td>relu</td>\n",
       "      <td>35</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.917021</td>\n",
       "      <td>2023-05-10 16:21:22.369381</td>\n",
       "      <td>2023-05-10 16:21:28.245127</td>\n",
       "      <td>0 days 00:00:05.875746</td>\n",
       "      <td>relu</td>\n",
       "      <td>8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>2.853811</td>\n",
       "      <td>2023-05-10 16:28:33.177632</td>\n",
       "      <td>2023-05-10 16:28:38.622553</td>\n",
       "      <td>0 days 00:00:05.444921</td>\n",
       "      <td>relu</td>\n",
       "      <td>82</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>3.012801</td>\n",
       "      <td>2023-05-10 16:28:38.739638</td>\n",
       "      <td>2023-05-10 16:28:44.558821</td>\n",
       "      <td>0 days 00:00:05.819183</td>\n",
       "      <td>relu</td>\n",
       "      <td>57</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>3.031410</td>\n",
       "      <td>2023-05-10 16:28:44.646746</td>\n",
       "      <td>2023-05-10 16:28:49.979856</td>\n",
       "      <td>0 days 00:00:05.333110</td>\n",
       "      <td>linear</td>\n",
       "      <td>87</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>3.005826</td>\n",
       "      <td>2023-05-10 16:28:50.066853</td>\n",
       "      <td>2023-05-10 16:28:55.117593</td>\n",
       "      <td>0 days 00:00:05.050740</td>\n",
       "      <td>relu</td>\n",
       "      <td>94</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>5.383955</td>\n",
       "      <td>2023-05-10 16:28:55.204335</td>\n",
       "      <td>2023-05-10 16:29:01.117522</td>\n",
       "      <td>0 days 00:00:05.913187</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>78</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "0        0  3.174398 2023-05-10 16:20:59.149699 2023-05-10 16:21:04.908337   \n",
       "1        1  5.852951 2023-05-10 16:21:05.009577 2023-05-10 16:21:10.817795   \n",
       "2        2  3.206507 2023-05-10 16:21:10.943480 2023-05-10 16:21:15.280943   \n",
       "3        3  3.118102 2023-05-10 16:21:15.357263 2023-05-10 16:21:22.269394   \n",
       "4        4  4.917021 2023-05-10 16:21:22.369381 2023-05-10 16:21:28.245127   \n",
       "..     ...       ...                        ...                        ...   \n",
       "95      95  2.853811 2023-05-10 16:28:33.177632 2023-05-10 16:28:38.622553   \n",
       "96      96  3.012801 2023-05-10 16:28:38.739638 2023-05-10 16:28:44.558821   \n",
       "97      97  3.031410 2023-05-10 16:28:44.646746 2023-05-10 16:28:49.979856   \n",
       "98      98  3.005826 2023-05-10 16:28:50.066853 2023-05-10 16:28:55.117593   \n",
       "99      99  5.383955 2023-05-10 16:28:55.204335 2023-05-10 16:29:01.117522   \n",
       "\n",
       "                 duration params_activation  params_neurons     state  \n",
       "0  0 days 00:00:05.758638              relu              51  COMPLETE  \n",
       "1  0 days 00:00:05.808218            linear              12  COMPLETE  \n",
       "2  0 days 00:00:04.337463              relu              92  COMPLETE  \n",
       "3  0 days 00:00:06.912131              relu              35  COMPLETE  \n",
       "4  0 days 00:00:05.875746              relu               8  COMPLETE  \n",
       "..                    ...               ...             ...       ...  \n",
       "95 0 days 00:00:05.444921              relu              82  COMPLETE  \n",
       "96 0 days 00:00:05.819183              relu              57  COMPLETE  \n",
       "97 0 days 00:00:05.333110            linear              87  COMPLETE  \n",
       "98 0 days 00:00:05.050740              relu              94  COMPLETE  \n",
       "99 0 days 00:00:05.913187           sigmoid              78  COMPLETE  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
